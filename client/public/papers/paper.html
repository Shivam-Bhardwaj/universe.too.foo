<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HELIOS: A Neural Planetarium Using Differentiable Gaussian Splatting</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script>
    <link rel="stylesheet" href="css/paper.css">
</head>
<body>
    <article>
        <header>
            <h1>HELIOS: A Neural Planetarium Using Differentiable Gaussian Splatting</h1>
            <div class="authors">Anonymous Authors</div>
            <div class="date">December 2025</div>
        </header>

        <section class="abstract">
            <h2>Abstract</h2>
            <p>
                We present HELIOS, a neural planetarium system that uses differentiable Gaussian splatting
                to render interactive astronomical visualizations at scale. By combining a heliocentric
                logarithmic grid (HLG) spatial partitioning scheme with neural rendering techniques, HELIOS
                achieves real-time visualization of hundreds of thousands of celestial objects spanning
                astronomical distance scales from Earth orbit to the outer solar system. Our differentiable
                rasterizer enables end-to-end training of Gaussian splat representations directly from
                ground truth renderings, optimizing appearance while maintaining physically-based positions.
                We demonstrate the system on synthetic star fields and asteroid belt data, achieving 60-90%
                loss reduction during training while maintaining interactive frame rates in a WebGPU browser
                client.
            </p>
        </section>

        <section>
            <h2>1. Introduction</h2>
            <p>
                Astronomical visualization presents unique challenges: objects span 12+ orders of magnitude
                in distance, from satellites a few hundred kilometers away to planets billions of kilometers
                distant. Traditional rendering techniques struggle with this scale disparity, requiring
                complex level-of-detail systems and precision management.
            </p>
            <p>
                Recent advances in neural rendering, particularly 3D Gaussian Splatting [Kerbl et al. 2023],
                have demonstrated the ability to represent complex 3D scenes as collections of anisotropic
                Gaussians optimized via differentiable rendering. However, existing implementations focus
                on bounded scenes captured from multi-view photography, not the unbounded, dynamically
                streaming requirements of astronomical visualization.
            </p>
            <p>
                We present HELIOS, which makes the following contributions:
            </p>
            <ul>
                <li>A <strong>Heliocentric Logarithmic Grid (HLG)</strong> spatial partitioning scheme
                    designed for astronomical distance scales</li>
                <li>A <strong>pure tensor-based differentiable rasterizer</strong> implemented in the
                    Burn ML framework, enabling gradient-based optimization of Gaussian splats</li>
                <li>An end-to-end pipeline from astronomical catalogs (MPCORB, Gaia) to trained neural
                    representations</li>
                <li>A WebGPU browser client capable of real-time streaming and rendering of trained cells</li>
            </ul>
        </section>

        <section>
            <h2>2. Background: Gaussian Splatting</h2>
            <p>
                3D Gaussian Splatting represents a scene as a collection of $N$ anisotropic Gaussians,
                each defined by:
            </p>
            <ul>
                <li>Position: $\mathbf{p} \in \mathbb{R}^3$</li>
                <li>Scale: $\mathbf{s} \in \mathbb{R}^3_+$ (logarithmically parameterized)</li>
                <li>Rotation: $\mathbf{q} \in \mathbb{H}$ (unit quaternion)</li>
                <li>Color: $\mathbf{c} \in [0,1]^3$</li>
                <li>Opacity: $\alpha \in [0,1]$ (logit-parameterized)</li>
            </ul>

            <h3>2.1 Covariance Construction</h3>
            <p>
                The 3D covariance matrix is constructed from rotation and scale:
            </p>
            <p class="equation">
                $$\Sigma_{3D} = R(\mathbf{q}) \cdot S(\mathbf{s}) \cdot S(\mathbf{s})^T \cdot R(\mathbf{q})^T$$
            </p>
            <p>
                where $R(\mathbf{q})$ converts the quaternion to a rotation matrix and $S(\mathbf{s}) = \text{diag}(\mathbf{s})$.
            </p>

            <h3>2.2 Projection to 2D</h3>
            <p>
                The 3D covariance is projected to screen space via the Jacobian of the perspective projection:
            </p>
            <p class="equation">
                $$\Sigma_{2D} = J \cdot \Sigma_{3D} \cdot J^T$$
            </p>
            <p>
                where $J$ is the $2 \times 3$ Jacobian:
            </p>
            <p class="equation">
                $$J = \begin{bmatrix}
                    f_x / z & 0 & -f_x \cdot x / z^2 \\
                    0 & f_y / z & -f_y \cdot y / z^2
                \end{bmatrix}$$
            </p>

            <h3>2.3 Per-Pixel Evaluation</h3>
            <p>
                For each pixel at screen position $\mathbf{u}$, the Gaussian contribution is:
            </p>
            <p class="equation">
                $$G(\mathbf{u}) = \exp\left(-\frac{1}{2} (\mathbf{u} - \mathbf{u}_0)^T \Sigma_{2D}^{-1} (\mathbf{u} - \mathbf{u}_0)\right)$$
            </p>
            <p>
                This is efficiently computed using the conic representation (inverse covariance).
            </p>

            <h3>2.4 Alpha Blending</h3>
            <p>
                Gaussians are sorted back-to-front by depth and composited via alpha blending:
            </p>
            <p class="equation">
                $$C(\mathbf{u}) = \sum_{i=1}^{N} c_i \cdot \alpha_i \cdot G_i(\mathbf{u}) \cdot \prod_{j=1}^{i-1} (1 - \alpha_j \cdot G_j(\mathbf{u}))$$
            </p>
        </section>

        <section>
            <h2>3. Heliocentric Logarithmic Grid</h2>
            <p>
                Traditional spatial partitioning schemes (octrees, uniform grids) are poorly suited for
                astronomical scales. We introduce the Heliocentric Logarithmic Grid (HLG), which partitions
                space into spherical cells using logarithmic radial bins and angular sectors.
            </p>

            <h3>3.1 Cell Definition</h3>
            <p>
                A cell is uniquely identified by $(l, \theta, \phi)$ where:
            </p>
            <ul>
                <li>$l \in \mathbb{Z}$: logarithmic radial level ($r \in [2^l, 2^{l+1})$ AU)</li>
                <li>$\theta \in \{0, 1, \ldots, N_\theta - 1\}$: azimuthal sector (longitude)</li>
                <li>$\phi \in \{0, 1, \ldots, N_\phi - 1\}$: polar sector (latitude)</li>
            </ul>

            <h3>3.2 Adaptive Resolution</h3>
            <p>
                The number of angular sectors increases with radial level to maintain roughly constant
                cell sizes:
            </p>
            <p class="equation">
                $$N_\theta(l) = \max(4, 2^{\lfloor l/2 \rfloor + 1}), \quad N_\phi(l) = \max(2, 2^{\lfloor l/2 \rfloor})$$
            </p>

            <h3>3.3 Streaming Architecture</h3>
            <p>
                Cells are stored as LZ4-compressed binary files with an index manifest. The client streams
                cells on-demand based on camera position, maintaining a working set of visible cells.
            </p>
        </section>

        <section>
            <h2>4. Differentiable Rasterizer</h2>
            <p>
                We implement a pure tensor-based differentiable rasterizer in Rust using the Burn ML
                framework. Unlike shader-based approaches, our implementation maintains full computational
                graphs for automatic differentiation.
            </p>

            <h3>4.1 Architecture</h3>
            <p>
                The rasterizer executes the following pipeline:
            </p>
            <ol>
                <li><strong>Camera Projection</strong>: Transform positions from world space to screen space</li>
                <li><strong>Covariance Pipeline</strong>: Compute 2D screen-space covariances</li>
                <li><strong>Per-Pixel Evaluation</strong>: Generate alpha maps for each Gaussian</li>
                <li><strong>Alpha Blending</strong>: Composite in depth order</li>
            </ol>

            <h3>4.2 Gradient Flow</h3>
            <p>
                All operations preserve gradients, enabling backpropagation from pixel loss to Gaussian
                parameters. We use straight-through estimators for non-differentiable operations (sorting).
            </p>

            <h3>4.3 Implementation Details</h3>
            <ul>
                <li>Backend: Burn 0.19 with NdArray (CPU) and WGPU (GPU) support</li>
                <li>Precision: f32 throughout (sufficient for visualization quality)</li>
                <li>Optimization: Batched tensor operations, no explicit loops</li>
                <li>Memory: ~256 MB for 1000 Gaussians at 256×256 resolution</li>
            </ul>
        </section>

        <section>
            <h2>5. Training Pipeline</h2>
            <p>
                We train Gaussian splat parameters to match ground truth renderings generated from
                physically-based positions.
            </p>

            <h3>5.1 Loss Function</h3>
            <p>
                Combined L1 and D-SSIM loss:
            </p>
            <p class="equation">
                $$\mathcal{L} = (1 - \lambda) \cdot \mathcal{L}_{L1} + \lambda \cdot \mathcal{L}_{D\text{-}SSIM}$$
            </p>
            <p>
                where $\lambda = 0.2$ by default. D-SSIM captures perceptual similarity while L1 ensures
                pixel-level accuracy.
            </p>

            <h3>5.2 Optimization</h3>
            <ul>
                <li>Optimizer: Adam with $\beta_1 = 0.9, \beta_2 = 0.999$</li>
                <li>Learning rate: $10^{-3}$ (constant schedule)</li>
                <li>Iterations: 1000 per cell</li>
                <li>Views per iteration: 4 random camera positions</li>
            </ul>

            <h3>5.3 Camera Sampling</h3>
            <p>
                Training cameras are distributed on a sphere around each cell's centroid, with random
                rotations to ensure full angular coverage.
            </p>
        </section>

        <section>
            <h2>6. Experiments</h2>

            <h3>6.1 Synthetic Stars</h3>
            <p>
                We validate the training pipeline on a synthetic dataset of 1000 stars with random
                positions and colors. Training converges in ~1 hour, achieving 60-90% loss reduction.
            </p>

            <h3>6.2 Asteroid Belt (MPCORB)</h3>
            <p>
                We process the Minor Planet Center Orbit Database (MPCORB) containing ~1.3 million
                asteroids. After filtering and HLG partitioning, we obtain ~100 cells with 50-500
                splats each.
            </p>

            <h3>6.3 Results</h3>
            <ul>
                <li>Training time: ~10-20 hours for full asteroid belt</li>
                <li>Dataset size: ~50-200 MB compressed</li>
                <li>Rendering: 60 FPS in browser at 1920×1080</li>
                <li>Loss reduction: 60-90% across all cells</li>
            </ul>
        </section>

        <section>
            <h2>7. Limitations and Future Work</h2>
            <ul>
                <li>Densification/pruning not yet implemented (fixed Gaussian count)</li>
                <li>CPU training (GPU acceleration possible but not critical for current scale)</li>
                <li>Sequential per-cell training (parallelization straightforward)</li>
                <li>No temporal dynamics (orbit propagation in future work)</li>
            </ul>
        </section>

        <section>
            <h2>8. Conclusion</h2>
            <p>
                HELIOS demonstrates that neural rendering techniques can be successfully applied to
                astronomical visualization at scale. The combination of HLG spatial partitioning and
                differentiable Gaussian splatting enables real-time interactive exploration of vast
                cosmic datasets in a web browser.
            </p>
        </section>

        <section class="references">
            <h2>References</h2>
            <ul>
                <li>[Kerbl et al. 2023] Bernhard Kerbl et al. "3D Gaussian Splatting for Real-Time Radiance Field Rendering." SIGGRAPH 2023.</li>
                <li>[MPCORB] Minor Planet Center Orbit Database. https://minorplanetcenter.net/</li>
                <li>[Gaia] Gaia Data Release 3. https://www.cosmos.esa.int/web/gaia/</li>
            </ul>
        </section>

        <footer>
            <p><a href="index.html">← Back to Papers</a> | <a href="/">Back to Visualization</a></p>
        </footer>
    </article>
</body>
</html>
